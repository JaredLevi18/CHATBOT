{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUePDtwRcGGx",
        "outputId": "a423073e-7b24-4d6e-fde4-97b30ef77a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyttsx3 in /usr/local/lib/python3.10/dist-packages (2.90)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyttsx3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9LCLdoxdboxS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn import Transformer, TransformerEncoder, TransformerEncoderLayer\n",
        "import json\n",
        "import pyttsx3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ga2D1wjRboxY"
      },
      "outputs": [],
      "source": [
        "# Training data from JSON file\n",
        "def load_training_data(file_path):\n",
        "    with open(file_path) as file:\n",
        "        data = json.load(file)\n",
        "    train_data = []\n",
        "    for item in data['data']:\n",
        "        for paragraph in item['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "            for qa in paragraph['qas']:\n",
        "                question = qa['question']\n",
        "                answers = [answer['text'] for answer in qa['answers']]\n",
        "                train_data.append((context, question, answers))\n",
        "    return train_data\n",
        "\n",
        "train_data_file = 'dev-v1.1.json'\n",
        "train_data = load_training_data(train_data_file)\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_data(data, max_seq_length):\n",
        "    src_data = []\n",
        "    tgt_data = []\n",
        "    vocab_token_to_id = {'<PAD>':0, '<UNK>':1}\n",
        "    for context, question, answers in data:\n",
        "        src_tokens = context.split()\n",
        "        tgt_tokens = answers[0].split()  # Considering only the first answer\n",
        "\n",
        "        # update the vocabulary with tokens\n",
        "        for token in src_tokens + tgt_tokens:\n",
        "            if token not in vocab_token_to_id:\n",
        "                vocab_token_to_id[token] = len(vocab_token_to_id)\n",
        "\n",
        "        # Truncate or pad the sequences to a fixed length\n",
        "        src_tokens = src_tokens[:max_seq_length]\n",
        "        tgt_tokens = tgt_tokens[:max_seq_length]\n",
        "\n",
        "        # convert tokens to token IDs\n",
        "        src_ids = ([vocab_token_to_id.get(token, vocab_token_to_id[\"<UNK>\"]) for token in src_tokens])\n",
        "        tgt_ids = ([vocab_token_to_id.get(token, vocab_token_to_id[\"<UNK>\"]) for token in tgt_tokens])\n",
        "        \n",
        "        # Pad sequences if they are shorter than the max length\n",
        "        src_ids = src_ids + [vocab_token_to_id[\"<PAD>\"]] * (max_seq_length - len(src_ids))\n",
        "        tgt_ids = tgt_ids + [vocab_token_to_id['<PAD>']] * (max_seq_length - len(tgt_ids)) \n",
        "\n",
        "        src_data.append(src_ids)\n",
        "        tgt_data.append(tgt_ids)\n",
        "    return src_data, tgt_data, vocab_token_to_id\n",
        "\n",
        "# Example usage with JSON data\n",
        "max_seq_length = 124\n",
        "src_data, tgt_data, vocab_token_to_id = preprocess_data(train_data, max_seq_length)\n",
        "src_data = torch.tensor(src_data)\n",
        "tgt_data = torch.tensor(tgt_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCMKbEoAboxZ",
        "outputId": "9a9b36b2-ab8a-45b8-cc7a-ebdc35f1d2f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 12, 15, 16, 17, 18,\n",
              "         19, 12, 20, 21, 22,  7, 16, 23, 24, 13, 25, 26, 27, 12, 15, 16, 23, 28,\n",
              "         13, 29, 30, 31, 10, 32, 33, 34,  2,  3, 35, 22,  9,  5, 36, 37, 38, 39,\n",
              "         40, 41, 42, 43, 44, 12, 45, 46, 47, 48, 41, 49, 50, 51, 52, 53,  5, 12,\n",
              "         54,  2, 55, 12, 56, 57, 12, 58, 59, 60, 61, 62, 63, 64, 65, 64, 66, 67,\n",
              "         12, 68, 14, 69, 70,  2,  3,  9, 60, 71, 72, 73, 74, 12,  9, 75, 76, 77,\n",
              "         78, 64, 79,  3, 80, 81, 82, 12, 83, 84, 85, 86, 12, 87, 72, 88]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_data[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bIqtFLJ7boxc"
      },
      "outputs": [],
      "source": [
        "# chatbot model\n",
        "class Chatbot(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim) -> None:\n",
        "        super(Chatbot, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.transformer_encoder_layer = TransformerEncoderLayer(input_dim, nhead=2)\n",
        "        self.transformer_encoder = TransformerEncoder(self.transformer_encoder_layer, num_layers=2)\n",
        "        self.decoder = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)  # encoder layer\n",
        "        # we change the shape of the tensor of tokens so the transformer can work\n",
        "        src = src.permute(1,0,2)   # (sequence_length, batch_size, embedding_size) ------> (batch_size, sequence_length, embedding_size)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = self.decoder(output.permute(1,0,2))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K15pxs6-boxd",
        "outputId": "54b712ae-7b5d-4740-c626-a5c0ec239443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "42586"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab_token_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sODKqM0jboxd"
      },
      "outputs": [],
      "source": [
        "# create the chatbot model\n",
        "input_dim = len(vocab_token_to_id)\n",
        "hidden_dim = 1 # 128\n",
        "output_dim = len(vocab_token_to_id)\n",
        "chatbot = Chatbot(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "# training \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(chatbot.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data in load_training_data:\n",
        "        inputs, labels = data\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Cast the inputs to half-precision\n",
        "        inputs = inputs.half()\n",
        "        with autocast():\n",
        "            # Forward pass\n",
        "            outputs = chatbot(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "        # Backward pass and gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "    print(f'Epoch: {epoch+1}, loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PVFBSfmNboxe"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17340\\4117599525.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchatbot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'chatbot_model.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# save the model\n",
        "torch.save(chatbot.state_dict(), 'chatbot_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "chatbot_model = Chatbot(input_dim, hidden_dim, output_dim)\n",
        "chatbot_model.load_state_dict(torch.load('chatbot_model.pt'))\n",
        "chatbot_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating responses\n",
        "def generate_response(input_text):\n",
        "    input_tokens = input_text.split()\n",
        "    input_ids = [vocab_token_to_id.get(token, vocab_token_to_id[\"<UNK>\"]) for token in input_tokens]\n",
        "    input_tensor = torch.tensor(input_ids).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = chatbot_model(input_tensor)\n",
        "        output_ids = torch.argmax(output, dim=2).squeeze(0).tolist()\n",
        "\n",
        "    response_tokens = [list(vocab_token_to_id.keys())[id] for id in output_ids]\n",
        "    response_text = \" \".join(response_tokens)\n",
        "\n",
        "    return response_text\n",
        "\n",
        "# Advanced response generation strategy (sample from the distribution instead of choosing the max)\n",
        "def generate_response_advanced(input_text, temperature=1.0):\n",
        "    input_tokens = input_text.split()\n",
        "    input_ids = [vocab_token_to_id.get(token, vocab_token_to_id[\"<UNK>\"]) for token in input_tokens]\n",
        "    input_tensor = torch.tensor(input_ids).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = chatbot_model(input_tensor)\n",
        "        logits = output.squeeze(0) / temperature\n",
        "        probabilities = nn.functional.softmax(logits, dim=-1)\n",
        "        sampled_ids = torch.multinomial(probabilities, num_samples=1).squeeze(1).tolist()\n",
        "\n",
        "    response_tokens = [list(vocab_token_to_id.keys())[id] for id in sampled_ids]\n",
        "    response_text = \" \".join(response_tokens)\n",
        "\n",
        "    return response_text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"Hello, how are you?\"\n",
        "response = generate_response(input_text)\n",
        "print(\"Chatbot Response:\", response)\n",
        "\n",
        "# Example usage with advanced response generation strategy\n",
        "input_text = \"Hello, how are you?\"\n",
        "response = generate_response_advanced(input_text, temperature=0.8)\n",
        "print(\"Advanced Chatbot Response:\", response)\n",
        "\n",
        "# Text-to-speech conversion\n",
        "def speak(text):\n",
        "    engine = pyttsx3.init()\n",
        "    engine.say(text)\n",
        "    engine.runAndWait()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
